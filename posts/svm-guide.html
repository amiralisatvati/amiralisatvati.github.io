<!DOCTYPE html>
<html lang='fa' dir='rtl'>
<head>
  <meta charset='UTF-8'>
  <meta name='viewport' content='width=device-width, initial-scale=1.0'>
  <title>ماشین بردار پشتیبان (SVM)</title>
</head>
<body>
  <header>
    <h1>ماشین بردار پشتیبان (SVM)</h1>
    <p>جزوه جامع برای درک شهودی و فرمولی SVM، مرز حاشیه ای، کرنل ها و کاربرد ها در طبقه بندی و رگرسیون.</p>
  </header>

  <main>
    <section>
      <h2>۱. ایده اصلی SVM</h2>
      <p>در ساده ترین حالت، SVM به دنبال یافتن یک خط یا ابرصفحه است که دو کلاس داده را طوری جدا کند که حاشیه بین آنها حداکثر شود[file:12].</p>
      <p>نقاطی که به این مرز نزدیک ترند و آن را تعیین می کنند به نام بردارهای پشتیبان شناخته می شوند و حذف آنها مرز تصمیم را تغییر می دهد، در حالی که حذف بقیه نقاط تاثیری بر مرز ندارد[file:12].</p>
    </section>

    <section>
      <h2>۲. هارد مارجین و سافت مارجین</h2>
      <p>در حالت هارد مارجین فرض می شود داده ها کاملا جدا پذیر هستند و هیچ نقطه ای اجازه عبور از مرز اشتباه را ندارد، اما این مدل نسبت به نویز بسیار حساس است[file:12].</p>
      <p>در حالت سافت مارجین، با معرفی متغیرهای خطا و پارامتر C مقدار کمی خطا پذیرفته می شود تا تعادل بهتری بین پهنای حاشیه و تعداد خطاها برقرار شود و مدل تعمیم پذیرتر شود[file:12].</p>
    </section>

    <section>
      <h2>۳. کرنل ها و فضاهای با بعد بالا</h2>
      <p>SVM می تواند با استفاده از توابع کرنل، داده های غیرخطی را در فضای ویژگی با بعد بالاتر به صورت خطی جدا پذیر کند بدون آنکه نگاشت را به طور صریح محاسبه کند[file:12].</p>
      <p>این ویژگی باعث می شود SVM در فضاهای با بعد بالا عملکرد بسیار خوبی داشته باشد، هرچند برای مجموعه داده های فوق العاده بزرگ از نظر محاسباتی پرهزینه است[file:12].</p>
    </section>

    <section>
      <h2>۴. رگرسیون بردار پشتیبان (SVR)</h2>
      <p>در SVR به جای طبقه بندی، هدف تقریب یک تابع عددی است؛ تابعی انتخاب می شود که همه نقاط حداکثر تا فاصله \( \varepsilon \) از آن قرار گیرند و پیچیدگی مدل حداقل شود[file:12].</p>
      <p>نقاطی که بیرون از لوله \( \varepsilon \) قرار می گیرند نقش مشابه بردارهای پشتیبان را بازی می کنند و در تعیین تابع نهایی موثرند[file:12].</p>
    </section>

    <section>
      <h2>۵. نکات امتحانی</h2>
      <ul>
        <li>هدف اصلی SVM ماکزیمم کردن حاشیه بین کلاس ها است، نه صرفا کمینه کردن خطا روی داده های آموزشی[file:12].</li>
        <li>معادله ابرصفحه تصمیم معمولا به صورت \( w^T x + b = 0 \) نوشته می شود[file:12].</li>
        <li>در تست های مفهومی، هر جا صحبت از دقت بالا در فضاهای با بعد زیاد باشد، SVM گزینه مناسبی است؛ در مقابل، برای داده های بسیار حجیم، هزینه محاسباتی آن به یک نقطه ضعف تبدیل می شود[file:12].</li>
      </ul>
    </section>
  </main>
</body>
</html>
